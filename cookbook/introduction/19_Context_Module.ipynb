{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Hawksight-AI/semantica/blob/main/cookbook/introduction/19_Context_Module.ipynb)\n",
    "\n",
    "# Deep Dive: Semantica Context Module (Architecture 2.0)\n",
    "\n",
    "## Overview\n",
    "\n",
    "The **Context Module** is the core state management system of Semantica. It allows agents to maintain coherent, persistent, and structured memory across long interactions. Unlike simple RAG systems that only use vector similarity, Semantica's Context Module combines:\n",
    "\n",
    "1.  **FastEmbed Integration**: High-performance, local embedding generation.\n",
    "2.  **Context Graph**: A structured knowledge graph for reasoning about relationships.\n",
    "3.  **Hierarchical Memory**: A tiered system with short-term (token-limited) and long-term (vector-backed) storage.\n",
    "4.  **Hybrid Retrieval**: Combining vector search, graph traversal (GraphRAG), and keyword matching.\n",
    "5.  **Persistence**: Full state serialization.\n",
    "\n",
    "This notebook provides a technical deep dive into these components.\n",
    "\n",
    "**Documentation**: [API Reference](https://semantica.readthedocs.io/reference/context/)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup and Configuration\n",
    "\n",
    "We need `semantica` and `fastembed` for local, high-speed embedding generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install semantica fastembed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "from semantica.context import AgentContext, ContextGraph, AgentMemory, EntityLinker\n",
    "from semantica.vector_store import VectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Vector Store with FastEmbed\n",
    "\n",
    "The `VectorStore` manages long-term memory. We will configure it to use **FastEmbed**, which runs efficient, quantized embedding models locally on the CPU.\n",
    "\n",
    "We use the `inmemory` backend for this demo, but Semantica supports Qdrant, Weaviate, and FAISS for production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize Vector Store\n",
    "vs = VectorStore(backend=\"inmemory\", dimension=384)\n",
    "\n",
    "# 2. Configure FastEmbed\n",
    "# We explicitly set the method to 'fastembed' and choose a lightweight, high-performance model.\n",
    "if hasattr(vs, \"embedder\") and vs.embedder:\n",
    "    print(\"Configuring VectorStore to use FastEmbed...\")\n",
    "    vs.embedder.set_text_model(\n",
    "        method=\"fastembed\", \n",
    "        model_name=\"BAAI/bge-small-en-v1.5\"\n",
    "    )\n",
    "\n",
    "# 3. Verify Embedding Generation\n",
    "text = \"Semantica enables complex agent behaviors.\"\n",
    "vector = vs.embed(text)\n",
    "print(f\"Generated embedding shape: {vector.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Context Graph Construction\n",
    "\n",
    "The `ContextGraph` stores structured data. While vectors capture *similarity*, graphs capture *relationships*.\n",
    "\n",
    "We will manually build a small graph to understand the API:\n",
    "- `add_node(node_id, node_type, content, **properties)`\n",
    "- `add_edge(source_id, target_id, edge_type, **properties)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg = ContextGraph()\n",
    "\n",
    "# Add Nodes\n",
    "# Note: 'content' is what is indexed for keyword search.\n",
    "kg.add_node(\n",
    "    node_id=\"user_alice\", \n",
    "    node_type=\"Person\", \n",
    "    content=\"Alice\", \n",
    "    role=\"Lead Engineer\"\n",
    ")\n",
    "kg.add_node(\n",
    "    node_id=\"tech_python\", \n",
    "    node_type=\"Technology\", \n",
    "    content=\"Python\", \n",
    "    version=\"3.11\"\n",
    ")\n",
    "kg.add_node(\n",
    "    node_id=\"project_semantica\", \n",
    "    node_type=\"Project\", \n",
    "    content=\"Semantica Framework\"\n",
    ")\n",
    "\n",
    "# Add Edges (Relationships)\n",
    "kg.add_edge(source_id=\"user_alice\", target_id=\"tech_python\", edge_type=\"USES\")\n",
    "kg.add_edge(source_id=\"tech_python\", target_id=\"project_semantica\", edge_type=\"POWERS\")\n",
    "\n",
    "# Traverse the Graph\n",
    "print(\"Neighbors of Python:\")\n",
    "neighbors = kg.get_neighbors(\"tech_python\")\n",
    "for n in neighbors:\n",
    "    # The neighbor dict contains the connected node info and the relationship that led to it\n",
    "    print(f\" - [Relationship: {n['relationship']}] -> {n['content']} ({n['type']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. AgentContext: The Unified Interface\n",
    "\n",
    "`AgentContext` combines the `VectorStore` and `ContextGraph` into a single system. It handles:\n",
    "1.  **Memory Management**: Routing inputs to short-term or long-term memory.\n",
    "2.  **Hybrid Retrieval**: Querying both vectors and the graph simultaneously.\n",
    "\n",
    "We will initialize it with limits to demonstrate the hierarchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = AgentContext(\n",
    "    vector_store=vs,\n",
    "    knowledge_graph=kg,\n",
    "    token_limit=500,       # Max tokens in Short-Term Memory (STM)\n",
    "    short_term_limit=5     # Max items in STM\n",
    ")\n",
    "\n",
    "# Store a new memory\n",
    "# This is automatically embedded (FastEmbed) and indexed.\n",
    "context.store(\n",
    "    content=\"Alice is optimizing the graph traversal algorithms in Semantica.\",\n",
    "    conversation_id=\"dev_sync_1\",\n",
    "    user_id=\"alice\"\n",
    ")\n",
    "\n",
    "# Retrieve Context\n",
    "# 'use_graph=True' enables GraphRAG: it finds entities in the query ('Alice') \n",
    "# and expands to their neighbors in the graph.\n",
    "results = context.retrieve(\n",
    "    query=\"What is Alice working on?\",\n",
    "    use_graph=True,\n",
    "    expand_graph=True\n",
    ")\n",
    "\n",
    "print(\"\\n--- Hybrid Retrieval Results ---\")\n",
    "for res in results:\n",
    "    print(f\"[{res['score']:.2f}] {res['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hierarchical Memory Management\n",
    "\n",
    "Watch how the `AgentContext` manages memory pressure. We defined `short_term_limit=5`.\n",
    "As we add more items, the oldest ones are flushed from the active buffer but remain safe in the Vector Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"STM Count (Start): {len(context.memory.short_term_memory)}\")\n",
    "\n",
    "# Fill up memory\n",
    "for i in range(1, 10):\n",
    "    context.store(f\"Log entry {i}: System status check.\")\n",
    "\n",
    "print(f\"STM Count (End): {len(context.memory.short_term_memory)}\")\n",
    "print(\"\\nCurrent Short-Term Memory Items:\")\n",
    "for item in context.memory.short_term_memory:\n",
    "    print(f\" - {item.content}\")\n",
    "\n",
    "# Notice that earlier log entries are gone from this list, \n",
    "# but they are still retrievable via search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Persistence\n",
    "\n",
    "To build stateful agents, we must save and load the context. \n",
    "**Crucial Step**: When loading, we must ensure the new `VectorStore` instance is configured with the same embedding model (`FastEmbed`) so vectors match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = \"./semantica_context_state\"\n",
    "\n",
    "# 1. Save state\n",
    "print(f\"Saving context to {SAVE_PATH}...\")\n",
    "context.save(SAVE_PATH)\n",
    "\n",
    "# 2. Initialize a fresh AgentContext\n",
    "print(\"Initializing fresh agent...\")\n",
    "new_kg = ContextGraph()\n",
    "new_vs = VectorStore(backend=\"inmemory\", dimension=384)\n",
    "\n",
    "# !!! IMPORTANT: Re-configure FastEmbed before loading !!!\n",
    "if hasattr(new_vs, \"embedder\") and new_vs.embedder:\n",
    "    new_vs.embedder.set_text_model(\n",
    "        method=\"fastembed\", \n",
    "        model_name=\"BAAI/bge-small-en-v1.5\"\n",
    "    )\n",
    "\n",
    "restored_context = AgentContext(vector_store=new_vs, knowledge_graph=new_kg)\n",
    "\n",
    "# 3. Load state\n",
    "restored_context.load(SAVE_PATH)\n",
    "\n",
    "# 4. Verify restoration\n",
    "print(f\"Restored STM items: {len(restored_context.memory.short_term_memory)}\")\n",
    "print(f\"Restored Graph nodes: {len(new_kg.nodes)}\")\n",
    "\n",
    "# Cleanup\n",
    "if os.path.exists(SAVE_PATH):\n",
    "    shutil.rmtree(SAVE_PATH)\n",
    "    print(\"Cleanup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You have successfully built a persistent, graph-aware, memory-managed context system using Semantica 2.0 components.\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Use `VectorStore` with `FastEmbed` for efficient local vectors.\n",
    "- Use `ContextGraph` to map relationships (`add_node`, `add_edge`).\n",
    "- Use `AgentContext` to manage the lifecycle of memories and retrieval.\n",
    "- Always configure your embedder on the fresh instance before calling `load()`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
